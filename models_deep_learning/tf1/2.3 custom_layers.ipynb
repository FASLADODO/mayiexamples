{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/eager/python/examples/notebooks/custom_layers.ipynb\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "tf.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layers: common sets of useful operations\n",
    "Most of the time when writing code for machine learning models you want to operate at a higher level of abstraction than individual operations and manipulation of individual variables.\n",
    "\n",
    "In the tf.keras.layers package, layers are objects. To construct a layer, simply construct the object. \n",
    "\n",
    "Most layers take as a first argument the number of output dimensions / channels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensorflow.python.keras.layers.core.Dense"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer = tf.keras.layers.Dense(100)  # 100指定了输出维度\n",
    "type(layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of input dimensions is often unnecessary, as it can be inferred the first time the layer is used, but it can be provided if you want to specify it manually, which is useful in some complex models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer = tf.keras.layers.Dense(10, input_shape=(None, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=50, shape=(10, 10), dtype=float32, numpy=\n",
       "array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]], dtype=float32)>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To use a layer, simply call it.\n",
    "layer(tf.zeros([10, 5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'dense_3/kernel:0' shape=(5, 10) dtype=float32, numpy=\n",
       " array([[-0.6242633 ,  0.36147523, -0.1431292 , -0.32359353,  0.01444876,\n",
       "          0.28876984, -0.12720239,  0.55321413,  0.3142482 , -0.0489282 ],\n",
       "        [-0.13301593, -0.0158633 ,  0.04746962,  0.0040918 , -0.40805823,\n",
       "          0.08727014, -0.4273607 , -0.47246763, -0.51467365, -0.4506082 ],\n",
       "        [-0.57103974,  0.18835014,  0.2003457 , -0.17082858, -0.28520125,\n",
       "          0.20919341,  0.34712416, -0.25119993,  0.10938066, -0.43051887],\n",
       "        [ 0.4026925 ,  0.02685809, -0.3404439 ,  0.23248672,  0.4420274 ,\n",
       "         -0.331929  , -0.5981807 ,  0.5543001 ,  0.10197508,  0.00768691],\n",
       "        [-0.18599209,  0.07625949,  0.12279665,  0.14112836, -0.5820571 ,\n",
       "         -0.28055677,  0.33070445,  0.13188833, -0.004888  , -0.52633286]],\n",
       "       dtype=float32)>,\n",
       " <tf.Variable 'dense_3/bias:0' shape=(10,) dtype=float32, numpy=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)>]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Layers have many useful methods. For example, you can inspect all variables\n",
    "# in a layer using `layer.variables` and trainable variables using\n",
    "# `layer.trainable_variables`. In this case a fully-connected layer\n",
    "# will have variables for weights and biases.\n",
    "layer.variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Variable 'dense_3/kernel:0' shape=(5, 10) dtype=float32, numpy=\n",
       " array([[-0.6242633 ,  0.36147523, -0.1431292 , -0.32359353,  0.01444876,\n",
       "          0.28876984, -0.12720239,  0.55321413,  0.3142482 , -0.0489282 ],\n",
       "        [-0.13301593, -0.0158633 ,  0.04746962,  0.0040918 , -0.40805823,\n",
       "          0.08727014, -0.4273607 , -0.47246763, -0.51467365, -0.4506082 ],\n",
       "        [-0.57103974,  0.18835014,  0.2003457 , -0.17082858, -0.28520125,\n",
       "          0.20919341,  0.34712416, -0.25119993,  0.10938066, -0.43051887],\n",
       "        [ 0.4026925 ,  0.02685809, -0.3404439 ,  0.23248672,  0.4420274 ,\n",
       "         -0.331929  , -0.5981807 ,  0.5543001 ,  0.10197508,  0.00768691],\n",
       "        [-0.18599209,  0.07625949,  0.12279665,  0.14112836, -0.5820571 ,\n",
       "         -0.28055677,  0.33070445,  0.13188833, -0.004888  , -0.52633286]],\n",
       "       dtype=float32)>,\n",
       " <tf.Variable 'dense_3/bias:0' shape=(10,) dtype=float32, numpy=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)>)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The variables are also accessible through nice accessors\n",
    "layer.kernel, layer.bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing custom layers\n",
    "TensorFlow 提供了便利的基类 tf.keras.layers.Layer。您可以通过继承它实现自己的层：\n",
    "\n",
    "The best way to implement your own layer is extending the `tf.keras.layers.Layer` class and implementing:\n",
    "\n",
    "* `__init__` , where you can do all input-independent initialization\n",
    "* `build`, where you know the shapes of the input tensors and can do the rest of the initialization\n",
    "* `call`, where you do the forward computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]], shape=(10, 10), dtype=float32)\n",
      "[<tf.Variable 'my_dense_layer_1/kernel:0' shape=(5, 10) dtype=float32, numpy=\n",
      "array([[ 0.5373625 ,  0.4484678 ,  0.5988926 ,  0.27661514,  0.49361652,\n",
      "         0.562203  ,  0.25794202, -0.6036055 ,  0.5633847 ,  0.48801762],\n",
      "       [-0.16616648, -0.37794575, -0.5100892 , -0.5995496 ,  0.5747586 ,\n",
      "        -0.22950682,  0.44296044, -0.6005403 ,  0.21082252, -0.52177525],\n",
      "       [ 0.05716884, -0.32144508, -0.6048095 ,  0.0656023 , -0.5737327 ,\n",
      "        -0.5962337 ,  0.10014254,  0.45625693, -0.26341763,  0.24943012],\n",
      "       [-0.15098336, -0.42382127, -0.07473701,  0.32558107, -0.06657565,\n",
      "        -0.5622134 , -0.33533457, -0.52376306, -0.06953794,  0.5037721 ],\n",
      "       [-0.40394196,  0.01500791,  0.39237076,  0.39057916, -0.29924452,\n",
      "        -0.61878604,  0.55274385,  0.11944157,  0.2898599 , -0.4646554 ]],\n",
      "      dtype=float32)>]\n"
     ]
    }
   ],
   "source": [
    "class MyDenseLayer(tf.keras.layers.Layer):\n",
    "  def __init__(self, num_outputs):\n",
    "    super(MyDenseLayer, self).__init__()\n",
    "    self.num_outputs = num_outputs\n",
    "\n",
    "  def build(self, input_shape):\n",
    "    self.kernel = self.add_variable(\"kernel\",\n",
    "                                    shape=[int(input_shape[-1]),\n",
    "                                           self.num_outputs])\n",
    "\n",
    "  def call(self, input):\n",
    "    return tf.matmul(input, self.kernel)\n",
    "\n",
    "layer = MyDenseLayer(10)\n",
    "print(layer(tf.zeros([10, 5])))\n",
    "print(layer.trainable_variables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models: composing layers\n",
    "Many interesting layer-like things in machine learning models are implemented by composing existing layers. For example, each residual block in a resnet is a composition of convolutions, batch normalizations, and a shortcut.\n",
    "\n",
    "The main class used when creating a layer-like thing which contains other layers is tf.keras.Model. Implementing one is done by inheriting from tf.keras.Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
